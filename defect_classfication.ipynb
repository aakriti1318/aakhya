{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectCSV:\n",
    "    def __init__(self, data_path, target_column):\n",
    "        self.data_path = data_path\n",
    "        self.target_column = target_column\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.xgb_model = None\n",
    "        self.rf_model = None\n",
    "\n",
    "    def load_data(self):\n",
    "        data = pd.read_csv(self.data_path)\n",
    "\n",
    "        # Calculate the percentage of missing values for each column\n",
    "        missing_percentage = data.isnull().sum() * 100 / len(data)\n",
    "\n",
    "        # Drop columns with more than 20% missing values\n",
    "        data = data.drop(columns=missing_percentage[missing_percentage > 20].index)\n",
    "\n",
    "        # Impute missing values\n",
    "        for col in data.columns:\n",
    "            if data[col].dtype.name == 'object':\n",
    "                data[col] = data[col].fillna(data[col].mode().iloc[0])\n",
    "            else:\n",
    "                data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "        self.X = data.drop(columns=[self.target_column])  # Replace 'target_column' with your actual target column name\n",
    "        self.y = data[self.target_column]\n",
    "\n",
    "        print('Preprocessing Data ..')\n",
    "        # Handle categorical and numerical features\n",
    "        categorical_cols = self.X.select_dtypes(include=['object']).columns\n",
    "        numerical_cols = self.X.select_dtypes(include=['number']).columns\n",
    "\n",
    "        # Encode categorical features\n",
    "        le = LabelEncoder()\n",
    "        self.X[categorical_cols] = self.X[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "\n",
    "        # Scale numerical features\n",
    "        scaler = StandardScaler()\n",
    "        self.X[numerical_cols] = scaler.fit_transform(self.X[numerical_cols])\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "\n",
    "    def train_xgboost(self):\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.1, 0.05, 0.01],\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "        print('Training XG Boost ..')\n",
    "        xgb_model = XGBClassifier()\n",
    "        grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        self.xgb_model = grid_search.best_estimator_\n",
    "\n",
    "    def train_random_forest(self):\n",
    "        param_dist = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "\n",
    "        print('Training Random Forest ..')\n",
    "        rf_model = RandomForestClassifier()\n",
    "        random_search = RandomizedSearchCV(rf_model, param_dist, n_iter=10, cv=5, scoring='accuracy')\n",
    "        random_search.fit(self.X_train, self.y_train)\n",
    "        self.rf_model = random_search.best_estimator_\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        print('Evaluating Models ..')\n",
    "        xgb_pred = self.xgb_model.predict(self.X_test)\n",
    "        rf_pred = self.rf_model.predict(self.X_test)\n",
    "\n",
    "        xgb_accuracy = accuracy_score(self.y_test, xgb_pred)\n",
    "        xgb_precision = precision_score(self.y_test, xgb_pred)\n",
    "        xgb_recall = recall_score(self.y_test, xgb_pred)\n",
    "        xgb_f1 = f1_score(self.y_test, xgb_pred)\n",
    "\n",
    "        rf_accuracy = accuracy_score(self.y_test, rf_pred)\n",
    "        rf_precision = precision_score(self.y_test, rf_pred)\n",
    "        rf_recall = recall_score(self.y_test, rf_pred)\n",
    "        rf_f1 = f1_score(self.y_test, rf_pred)\n",
    "\n",
    "        print(\"XGBoost Metrics:\")\n",
    "        print(\"Accuracy:\", xgb_accuracy)\n",
    "        print(\"Precision:\", xgb_precision)\n",
    "        print(\"Recall:\", xgb_recall)\n",
    "        print(\"F1-Score:\", xgb_f1)\n",
    "\n",
    "        print(\"\\nRandom Forest Metrics:\")\n",
    "        print(\"Accuracy:\", rf_accuracy)\n",
    "        print(\"Precision:\", rf_precision)\n",
    "        print(\"Recall:\", rf_recall)\n",
    "        print(\"F1-Score:\", rf_f1)\n",
    "\n",
    "        # Choose the model with the best performance based on your chosen metric\n",
    "        best_model = self.xgb_model if xgb_f1 > rf_f1 else self.rf_model\n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "class DefectIMG:\n",
    "    def __init__(self, train_dir, test_dir, image_size=(224, 224), batch_size=32, grayscale=False):\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.grayscale = grayscale\n",
    "        self.model = None\n",
    "        self.class_labels = None\n",
    "        self.input_shape = (image_size[0], image_size[1], 1) if grayscale else (image_size[0], image_size[1], 3)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        color_mode = 'grayscale' if self.grayscale else 'rgb'\n",
    "\n",
    "        # Data augmentation for training set\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=30,\n",
    "            width_shift_range=0.3,\n",
    "            height_shift_range=0.3,\n",
    "            shear_range=0.3,\n",
    "            zoom_range=0.3,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "        # Simple preprocessing for validation and test set\n",
    "        val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "        self.train_generator = train_datagen.flow_from_directory(\n",
    "            self.train_dir,\n",
    "            target_size=self.image_size,\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical',\n",
    "            color_mode=color_mode\n",
    "        )\n",
    "\n",
    "        self.test_generator = val_datagen.flow_from_directory(\n",
    "            self.test_dir,\n",
    "            target_size=self.image_size,\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='categorical',\n",
    "            color_mode=color_mode,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        # Save class labels for later reference\n",
    "        self.class_labels = self.train_generator.class_indices\n",
    "\n",
    "    def build_model(self, num_classes):\n",
    "        base_model = ResNet152(include_top=False, weights='imagenet', input_shape=self.input_shape)\n",
    "\n",
    "        # Fine-tune the last few layers\n",
    "        for layer in base_model.layers[:-20]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Add custom classification layers\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "        self.model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    def train_model(self, epochs):\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "        # Adding EarlyStopping to avoid overfitting\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        self.model.fit(\n",
    "            self.train_generator,\n",
    "            steps_per_epoch=len(self.train_generator),\n",
    "            epochs=epochs,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        # Preprocess the image\n",
    "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=self.image_size, color_mode='grayscale' if self.grayscale else 'rgb')\n",
    "        x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x /= 255.0\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(x)\n",
    "        predicted_class_index = np.argmax(prediction)\n",
    "\n",
    "        # Get the actual class name using the saved labels\n",
    "        predicted_class_name = list(self.class_labels.keys())[predicted_class_index]\n",
    "        return predicted_class_name\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        loss, accuracy = self.model.evaluate(self.test_generator, steps=len(self.test_generator))\n",
    "        print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalClassifier:\n",
    "    def __init__(self, csv_path, image_dir, target_column):\n",
    "        self.csv_path = csv_path\n",
    "        self.image_dir = image_dir\n",
    "        self.target_column = target_column\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.defect_classifier_img = None\n",
    "        self.defect_classifier_csv = None\n",
    "        self.best_model = None\n",
    "\n",
    "    def load_data(self):\n",
    "        print('Loading Data ..')\n",
    "        \n",
    "        # Load CSV data\n",
    "        data = pd.read_csv(self.csv_path)\n",
    "\n",
    "        # Create a DefectIMG instance\n",
    "        self.defect_classifier_img = DefectIMG(f\"{self.image_dir}/train\", f\"{self.image_dir}/val\")\n",
    "        self.defect_classifier_img.preprocess_data()\n",
    "        self.defect_classifier_img.build_model(num_classes=2)  # Assuming binary classification for defect\n",
    "        self.defect_classifier_img.train_model(epochs=10)\n",
    "\n",
    "        # Predict defect classes for images in the CSV\n",
    "        image_paths = data['image_path']  # Replace 'image_path' with your actual column name\n",
    "        defect_predictions = []\n",
    "        for path in image_paths:\n",
    "            defect_predictions.append(self.defect_classifier_img.predict(path))\n",
    "        data['defect_prediction'] = defect_predictions\n",
    "\n",
    "        self.multi_modal_data = 'multi_modal_data.csv'\n",
    "        data.to_csv(self.multi_modal_data, index=False)\n",
    "\n",
    "    def defect_classification(self):\n",
    "        print('Defect Classification on CSV ..')\n",
    "        self.defect_classifier_csv = DefectCSV(self.multi_modal_data, self.target_column)\n",
    "        self.defect_classifier_csv.load_data()\n",
    "        self.defect_classifier_csv.train_random_forest()\n",
    "        self.defect_classifier_csv.train_xgboost()\n",
    "        self.best_model = self.defect_classifier_csv.evaluate_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Data ..\n",
      "Training Random Forest ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BhavishyaPandit\\Desktop\\VSC Projects\\FlawFinder\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\BhavishyaPandit\\Desktop\\VSC Projects\\FlawFinder\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\BhavishyaPandit\\Desktop\\VSC Projects\\FlawFinder\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\BhavishyaPandit\\Desktop\\VSC Projects\\FlawFinder\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\BhavishyaPandit\\Desktop\\VSC Projects\\FlawFinder\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\BhavishyaPandit\\Desktop\\VSC Projects\\FlawFinder\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.83989183 0.83989183 0.96180731        nan        nan\n",
      " 0.95833389        nan 0.83989183        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XG Boost ..\n",
      "Evaluating Models ..\n",
      "XGBoost Metrics:\n",
      "Accuracy: 0.9552469135802469\n",
      "Precision: 0.9591474245115453\n",
      "Recall: 0.989010989010989\n",
      "F1-Score: 0.9738503155996393\n",
      "\n",
      "Random Forest Metrics:\n",
      "Accuracy: 0.9552469135802469\n",
      "Precision: 0.9591474245115453\n",
      "Recall: 0.989010989010989\n",
      "F1-Score: 0.9738503155996393\n"
     ]
    }
   ],
   "source": [
    "path = 'manufacturing_defect_dataset.csv'\n",
    "target_column = 'DefectStatus'\n",
    "csv_obj = DefectCSV(path, target_column)\n",
    "csv_obj.load_data()\n",
    "csv_obj.train_random_forest()\n",
    "csv_obj.train_xgboost()\n",
    "best_model = csv_obj.evaluate_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 656 images belonging to 2 classes.\n",
      "Found 164 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step - accuracy: 0.5139 - loss: 0.7002\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.6610 - loss: 0.6119\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.7059 - loss: 0.5555\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.6586 - loss: 0.5834\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.6892 - loss: 0.5642\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.7110 - loss: 0.5668\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 919us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.7252 - loss: 0.5293\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.7012 - loss: 0.5520\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 17/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.7248 - loss: 0.5322\n",
      "Epoch 18/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 745us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 19/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.7382 - loss: 0.5037\n",
      "Epoch 20/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 21/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.7198 - loss: 0.5087\n",
      "Epoch 22/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 23/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.7009 - loss: 0.5147\n",
      "Epoch 24/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 25/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.7443 - loss: 0.5063\n",
      "Epoch 26/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 27/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - accuracy: 0.7320 - loss: 0.5019\n",
      "Epoch 28/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 29/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.7445 - loss: 0.4940\n",
      "Epoch 30/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.2847 - loss: 0.8394\n",
      "Test Accuracy: 50.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directories for training, validation, and test data\n",
    "train_dir = 'Breast Cancer New/Train'\n",
    "test_dir = 'Breast Cancer New/Test'\n",
    "\n",
    "# Initialize the DefectIMG class\n",
    "defect_img_model = DefectIMG(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    image_size=(224, 224),  # Image size as expected by ResNet152\n",
    "    batch_size=32,          # Batch size\n",
    "    grayscale=False         # Set to True if you want to convert images to grayscale\n",
    ")\n",
    "\n",
    "# Preprocess the data (including data augmentation and normalization)\n",
    "defect_img_model.preprocess_data()\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(defect_img_model.class_labels)\n",
    "\n",
    "# Build the model with the specified number of classes\n",
    "defect_img_model.build_model(num_classes=num_classes)\n",
    "\n",
    "# Train the model with a specified number of epochs\n",
    "defect_img_model.train_model(epochs=30)\n",
    "\n",
    "# Evaluate the model on the test set and print accuracy\n",
    "defect_img_model.evaluate_model()\n",
    "\n",
    "# Example of predicting a single image\n",
    "# image_path = 'path/to/single_image.jpg'\n",
    "# predicted_class = defect_img_model.predict(image_path)\n",
    "# print(f\"The predicted class for the image is: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
